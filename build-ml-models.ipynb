{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74047788-1ae9-4960-8342-b3a4e2b21f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ada398-a21a-417c-9954-f0ed48d5d49b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Spark application\n",
    "spark = SparkSession.builder.master('local[*]').appName('ML Model Development').getOrCreate()\n",
    "\n",
    "# Setting AWS Access Credentials\n",
    "spark.conf.set(\"fs.s3a.access.key\", str(os.environ['AWS_ACCESS_KEY']))\n",
    "spark.conf.set(\"fs.s3a.secret.key\", str(os.environ['AWS_SECRET_ACCESS_KEY']))\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab696365-6635-47bf-87cb-1af13e3bb666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- trip_duration: long (nullable = true)\n",
      " |-- weekend: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- dew: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- visibility: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read merged final dataset from S3\n",
    "\n",
    "DATASET_PATH = 's3://data228/final-data/*.parquet' \"path\"\n",
    "df = spark.read.parquet(DATASET_PATH)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ffad1c7-af98-47e2-9d48-be858dfea1d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing required libraries for ML models development\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, FMRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cea963c-3925-4d24-8422-a4a5e9612d4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble valid features for training\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['year', 'month', 'date', 'hour', 'passenger_count', 'trip_distance', 'trip_duration', 'PULocationID', 'DOLocationID', 'weekend', 'temp', 'dew', 'humidity', 'windspeed', 'visibility'],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=2023)\n",
    "\n",
    "# Path to save trained models\n",
    "MODELS_PATH = \"s3://data228/trained-models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0502872-ad34-45e6-a0a8-51414225d6d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca8bb21-811c-41cf-b383-f10ae887a79f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regressor R2 score: 0.9098464932059501\n",
      "Linear Regressor (RMSE): 4.184220896046416\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='total_amount')\n",
    "\n",
    "# Train model\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "r2_evaluator = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='r2')\n",
    "loss_evaluator = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# Calculate accuracy and RMSE loss\n",
    "lr_loss = loss_evaluator.evaluate(lr_predictions)\n",
    "lr_accuracy = r2_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"Linear Regressor R2 score: {lr_accuracy}\")\n",
    "print(f\"Linear Regressor (RMSE): {lr_loss}\")\n",
    "\n",
    "# Save the model into S3 for future predictions\n",
    "lr_model.write().overwrite().save(MODELS_PATH + \"lr_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baff5e87-2966-44dd-867a-45a96adbf0eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "affe1ae1-32f4-48e8-baae-3a0e663bb0cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor R2 score: 0.8893802623505866\n",
      "Random Forest Regressor (RMSE): 4.634891838980823\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='total_amount')\n",
    "\n",
    "# Train model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "r2_evaluator = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='r2')\n",
    "loss_evaluator = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# Calculate accuracy and RMSE loss\n",
    "rf_accuracy = r2_evaluator.evaluate(rf_predictions)\n",
    "rf_loss = loss_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest Regressor R2 score: {rf_accuracy}\")\n",
    "print(f\"Random Forest Regressor (RMSE): {rf_loss}\")\n",
    "\n",
    "# Save the model into S3 for future predictions\n",
    "rf_model.write().overwrite().save(MODELS_PATH + \"rf_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "289448e9-af09-4aba-8b9d-61e5137c64ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Gradient Boosted Trees Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efeda8f3-e468-4283-bfcb-7127c3634024",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT Regressor R2 score: 0.9167801313839348\n",
      "GBT Regressor (RMSE): 4.02009952172111\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression model\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='total_amount')\n",
    "\n",
    "# Train model\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "r2_evaluator = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='r2')\n",
    "loss_evaluator = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# Calculate accuracy and RMSE loss\n",
    "gbt_loss = loss_evaluator.evaluate(gbt_predictions)\n",
    "gbt_accuracy = r2_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"GBT Regressor R2 score: {gbt_accuracy}\")\n",
    "print(f\"GBT Regressor (RMSE): {gbt_loss}\")\n",
    "\n",
    "# Save the model into S3 for future predictions\n",
    "gbt_model.write().overwrite().save(MODELS_PATH + \"gbt_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a4de253-3557-4788-8742-f4cef0822d74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "build-ml-models",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
